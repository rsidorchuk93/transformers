{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsidorchuk93/transformers/blob/main/Transformers_attention_encoding_theory_and_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm implementing attention mechanism in transformers"
      ],
      "metadata": {
        "id": "PtvtwzmfxCQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory:\n",
        "\n",
        "\n",
        "1.   **What types of optimizers cost function exist? Gradient descent is a simple one, what others?** Adam an adaptive optimization algorithm that combines ideas from both momentum and RMSprop optimization methods. Adam adapts the learning rate for each parameter based on the first and second moments of the gradients, allowing it to converge faster and more reliably than other optimization methods.\n",
        "2.   **Difference between autoregression and bi-directional encoder.** Autoregression is a specific technique for time series forecasting that relies on previous values to predict the next value in the sequence, while a bi-directional encoder is a type of neural network architecture that allows the model to take into account both past and future context when making predictions.\n",
        "3.   **Objective of encoder and decoder**. The encoder's objective is to transform the input data into a lower-dimensional feature representation, while the decoder's objective is to reconstruct the original input data from the compressed representation produced by the encoder. The autoencoder neural network's objective is to learn a compressed representation of the input data that captures the most important information while being able to reconstruct the original input data with minimal error.\n",
        "4.   **What loss function transformers use.** The cross-entropy loss is a measure of the difference between the predicted probability distribution (output of the model) and the true probability distribution (ground truth).\n",
        "The softmax function is typically used in conjunction with the cross-entropy loss to convert the output of the model, which is a vector of scores, into a probability distribution over the vocabulary of words. The softmax function normalizes the scores so that they sum to 1, allowing them to be interpreted as probabilities.\n",
        "5.   **Positional encoding - how it is calculated** Positional encoding is a technique used in transformer-based neural networks to add information about the relative position of tokens in a sequence. This allows the model to better understand the order of the sequence. Positional encoding consists of a set of sine and cosine functions with different frequencies and phases, which are added to the embedding vector of each token in the sequence. The frequency of each sine and cosine function depends on the position of the token and the dimension of the embedding vector. The amplitude of the functions decreases as the frequency increases, allowing the model to capture fine-grained positional information for low-frequency functions and coarse-grained positional information for high-frequency functions."
      ],
      "metadata": {
        "id": "DE6L50Eq0q9F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQuDC4_Tw9fB"
      },
      "outputs": [],
      "source": [
        "# importing libraries, can't use pytorch\n",
        "import numpy as np\n",
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenize input text\n",
        "def tokenize(text):\n",
        "  tokens = list(set(text))  \n",
        "  return tokens\n",
        "\n",
        "tokens = tokenize('My name is Roman')\n",
        "print(tokens)\n",
        "\n",
        "\n",
        "# additional step is to position embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKIndrm-xMbV",
        "outputId": "13b753ed-e343-42d9-d58a-fc2e269eb54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['M', 'n', 'm', 'e', 'y', ' ', 'i', 'o', 'R', 's', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Calculate embeddings\n",
        "# define embeddings dimension\n",
        "emb_dim = 30\n",
        "def embeddings(tokens, emb_dim):\n",
        "  # initialize embedding matrix with random values\n",
        "  emb_matrix = np.random.randn(len(tokens), emb_dim)\n",
        "\n",
        "  # loop over tokens and update embedding matrix\n",
        "  for i, token in enumerate(tokens):\n",
        "    # calculate embeddings for characters\n",
        "    char_embeddings = []\n",
        "    for char in token:\n",
        "      # initialize char embedding with random values\n",
        "      char_emb = np.random.randn(emb_dim)\n",
        "      char_embeddings.append(char_emb)\n",
        "\n",
        "    # calculate mean\n",
        "    token_emb = np.mean(char_embeddings, axis = 0)\n",
        "\n",
        "    # update embedding matrix\n",
        "    emb_matrix[i] = token_emb\n",
        "\n",
        "  # calculate emb mean and return embedded\n",
        "  embed = np.mean(emb_matrix, axis = 0)\n",
        "  return embed\n",
        "\n",
        "embed = embeddings(tokens, emb_dim)\n",
        "embed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvNuMKWuxYtl",
        "outputId": "3745bb3f-146c-4012-d54e-11fb0d520474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.61304224, -0.19007164,  0.16118197, -0.55563408, -0.35936641,\n",
              "        0.26078099,  0.16680578, -0.07712182, -0.1242666 ,  0.00095757,\n",
              "       -0.41096915, -0.22245473, -0.41146693, -0.05862648,  0.05245742,\n",
              "       -0.37957559, -0.46196666,  0.27357544, -0.44123281, -0.11279968,\n",
              "       -0.34191517, -0.09971721, -0.03584275, -0.12380152, -0.12210476,\n",
              "       -0.17348563, -0.20965604,  0.38626767,  0.4699147 ,  0.0538929 ])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Implement self attention from embeddings\n",
        "\n",
        "def self_attention(embed):\n",
        "  # dot product of embeddigns with itself\n",
        "  dot_product = np.dot(embed, embed.T)\n",
        "\n",
        "  # apply softmax \n",
        "  attn_weights = softmax(dot_product, axis = -1)\n",
        "\n",
        "  # weighted sum\n",
        "  weighted_sum = np.dot(attn_weights, embed)\n",
        "\n",
        "  # normzlize the weighted sum\n",
        "  norm_weighted_sum = weighted_sum / np.sum(attn_weights)\n",
        "\n",
        "  return norm_weighted_sum\n",
        "\n",
        "\n",
        "self_attention(embed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2FberSX0KUo",
        "outputId": "0864054d-731a-4178-8455-99929383e868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.61304224, -0.19007164,  0.16118197, -0.55563408, -0.35936641,\n",
              "        0.26078099,  0.16680578, -0.07712182, -0.1242666 ,  0.00095757,\n",
              "       -0.41096915, -0.22245473, -0.41146693, -0.05862648,  0.05245742,\n",
              "       -0.37957559, -0.46196666,  0.27357544, -0.44123281, -0.11279968,\n",
              "       -0.34191517, -0.09971721, -0.03584275, -0.12380152, -0.12210476,\n",
              "       -0.17348563, -0.20965604,  0.38626767,  0.4699147 ,  0.0538929 ])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the above self-attention layer is followed by feed-forward network network, each is followed by layer normalization\n",
        "\n",
        "encoder takes as inputs:\n",
        "\n",
        "1.   number of self-attention heads\n",
        "\n",
        "2.   dropout probability for forward-feed network\n",
        "\n",
        "3.   feed forward dimension, number of layers, and hidden dimension"
      ],
      "metadata": {
        "id": "CGO1N_RPGDwJ"
      }
    }
  ]
}